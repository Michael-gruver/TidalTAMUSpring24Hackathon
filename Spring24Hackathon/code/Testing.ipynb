{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"romVHSIkYPoQ","executionInfo":{"status":"ok","timestamp":1708883306663,"user_tz":360,"elapsed":1252,"user":{"displayName":"Juan Gomez Sandoval","userId":"07136595439501174749"}},"outputId":"1795b7b0-fe69-43c0-bd83-2400f999082c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wh07xWvCegS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708793969642,"user_tz":360,"elapsed":850,"user":{"displayName":"Michael Gruver","userId":"02047184803156616712"}},"outputId":"8d784c4b-bb45-43d5-db68-8cb253800e13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from network\n","\n","import os\n","from PIL import Image\n","import cv2\n","import gdown\n","import argparse\n","import numpy as np\n","\n","import torch\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","\n","from collections import OrderedDict\n","from options import opt\n","\n","import scipy.cluster\n","from webcolors import rgb_to_name\n","\n","\n","\n","def load_checkpoint(model, checkpoint_path):\n","    if not os.path.exists(checkpoint_path):\n","        print(\"----No checkpoints at given path----\")\n","        return\n","    model_state_dict = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))\n","    new_state_dict = OrderedDict()\n","    for k, v in model_state_dict.items():\n","        name = k[7:]  # remove `module.`\n","        new_state_dict[name] = v\n","\n","    model.load_state_dict(new_state_dict)\n","    print(\"----checkpoints loaded from path: {}----\".format(checkpoint_path))\n","    return model\n","\n","\n","def get_palette(num_cls):\n","    \"\"\" Returns the color map for visualizing the segmentation mask.\n","    Args:\n","        num_cls: Number of classes\n","    Returns:\n","        The color map\n","    \"\"\"\n","    n = num_cls\n","    palette = [0] * (n * 3)\n","    for j in range(0, n):\n","        lab = j\n","        palette[j * 3 + 0] = 0\n","        palette[j * 3 + 1] = 0\n","        palette[j * 3 + 2] = 0\n","        i = 0\n","        while lab:\n","            palette[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\n","            palette[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\n","            palette[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\n","            i += 1\n","            lab >>= 3\n","    return palette\n","\n","\n","class Normalize_image(object):\n","    \"\"\"Normalize given tensor into given mean and standard dev\n","\n","    Args:\n","        mean (float): Desired mean to substract from tensors\n","        std (float): Desired std to divide from tensors\n","    \"\"\"\n","\n","    def __init__(self, mean, std):\n","        assert isinstance(mean, (float))\n","        if isinstance(mean, float):\n","            self.mean = mean\n","\n","        if isinstance(std, float):\n","            self.std = std\n","\n","        self.normalize_1 = transforms.Normalize(self.mean, self.std)\n","        self.normalize_3 = transforms.Normalize([self.mean] * 3, [self.std] * 3)\n","        self.normalize_18 = transforms.Normalize([self.mean] * 18, [self.std] * 18)\n","\n","    def __call__(self, image_tensor):\n","        if image_tensor.shape[0] == 1:\n","            return self.normalize_1(image_tensor)\n","\n","        elif image_tensor.shape[0] == 3:\n","            return self.normalize_3(image_tensor)\n","\n","        elif image_tensor.shape[0] == 18:\n","            return self.normalize_18(image_tensor)\n","\n","        else:\n","            assert \"Please set proper channels! Normlization implemented only for 1, 3 and 18\"\n","\n","\n","\n","\n","def apply_transform(img):\n","    transforms_list = []\n","    transforms_list += [transforms.ToTensor()]\n","    transforms_list += [Normalize_image(0.5, 0.5)]\n","    transform_rgb = transforms.Compose(transforms_list)\n","    return transform_rgb(img)\n","\n","\n","\n","def generate_mask(input_image, net, palette, device = 'cpu'):\n","\n","    # img = Image.open(input_image).convert('RGB')\n","    img = input_image\n","    img_size = img.size\n","    img = img.resize((768, 768), Image.BICUBIC)\n","    image_tensor = apply_transform(img)\n","    image_tensor = torch.unsqueeze(image_tensor, 0)\n","\n","    alpha_out_dir = os.path.join(opt.output,'alpha')\n","    cloth_seg_out_dir = os.path.join(opt.output,'cloth_seg')\n","\n","    os.makedirs(alpha_out_dir, exist_ok=True)\n","    os.makedirs(cloth_seg_out_dir, exist_ok=True)\n","\n","    with torch.no_grad():\n","        output_tensor = net(image_tensor.to(device))\n","        output_tensor = F.log_softmax(output_tensor[0], dim=1)\n","        output_tensor = torch.max(output_tensor, dim=1, keepdim=True)[1]\n","        output_tensor = torch.squeeze(output_tensor, dim=0)\n","        output_arr = output_tensor.cpu().numpy()\n","\n","    classes_to_save = []\n","\n","    # Check which classes are present in the image\n","    for cls in range(1, 4):  # Exclude background class (0)\n","        if np.any(output_arr == cls):\n","            classes_to_save.append(cls)\n","\n","    # Save alpha masks\n","    for cls in classes_to_save:\n","        alpha_mask = (output_arr == cls).astype(np.uint8) * 255\n","        alpha_mask = alpha_mask[0]  # Selecting the first channel to make it 2D\n","        the_image = alpha_mask.reshape(768, 768, 1) * img * -1\n","        # Create an image object from the array\n","        the_img = Image.fromarray(the_image.astype('uint8'), 'RGB')\n","        # Save the image object\n","        the_img.save(os.path.join(alpha_out_dir, f'image{cls}.png'))\n","        alpha_mask_img = Image.fromarray(alpha_mask, mode='L')\n","        alpha_mask_img = alpha_mask_img.resize(img_size, Image.BICUBIC)\n","        alpha_mask_img.save(os.path.join(alpha_out_dir, f'{cls}.png'))\n","\n","    # Save final cloth segmentations\n","    cloth_seg = Image.fromarray(output_arr[0].astype(np.uint8), mode='P')\n","    cloth_seg.putpalette(palette)\n","    cloth_seg = cloth_seg.resize(img_size, Image.BICUBIC)\n","    cloth_seg.save(os.path.join(cloth_seg_out_dir, 'final_seg.png'))\n","    return cloth_seg\n","\n","\n","\n","def check_or_download_model(file_path):\n","    if not os.path.exists(file_path):\n","        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","        url = \"https://drive.google.com/uc?id=11xTBALOeUkyuaK3l60CpkYHLTmv7k3dY\"\n","        gdown.download(url, file_path, quiet=False)\n","        print(\"Model downloaded successfully.\")\n","    else:\n","        print(\"Model already exists.\")\n","\n","\n","def load_seg_model(checkpoint_path, device='cpu'):\n","    net = U2NET(in_ch=3, out_ch=4)\n","    check_or_download_model(checkpoint_path)\n","    net = load_checkpoint(net, checkpoint_path)\n","    net = net.to(device)\n","    net = net.eval()\n","\n","    return net\n","\n","\n","def main(args):\n","\n","    device = 'cuda:0' if args.cuda else 'cpu'\n","\n","    # Create an instance of your model\n","    model = load_seg_model(args.checkpoint_path, device=device)\n","\n","    palette = get_palette(4)\n","\n","    img = Image.open(args.image).convert('RGB')\n","\n","    cloth_seg = generate_mask(img, net=model, palette=palette, device=device)\n","\n","def convert_rgb_to_name(rgb_tuple):\n","    try:\n","        named_color = rgb_to_name(rgb_tuple, spec='css3')\n","        return f\"The closest color name is: {named_color}\"\n","    except ValueError:\n","        return \"No defined color name found for this RGB value.\"\n","\n","def get_dominant_color(image_path):\n","    # Open the image\n","    img = Image.open(image_path)\n","\n","    # Resize the image (optional, for faster processing)\n","    #img = img.resize((150, 150))\n","\n","    # Convert the image to an array of RGB values\n","    ar = np.asarray(img)\n","    shape = ar.shape\n","    ar = ar.reshape(np.product(shape[:2]), shape[2]).astype(float)\n","\n","\n","    # Filter out white pixels (assuming RGB value for white is [255, 255, 255])\n","    non_white_indices = np.any(ar > 100, axis=1)\n","    ar_non_white = ar[non_white_indices]\n","\n","\n","    if len(ar_non_white) == 0:\n","        # If there are no non-white pixels, return None\n","        return None\n","\n","\n","    # Perform k-means clustering\n","    NUM_CLUSTERS = 7  # You can adjust this value\n","    codes, _ = scipy.cluster.vq.kmeans(ar_non_white, NUM_CLUSTERS)\n","\n","    # Find the most frequent color\n","    vecs, _ = scipy.cluster.vq.vq(ar_non_white, codes)\n","    counts, _ = np.histogram(vecs, len(codes))\n","    index_max = np.argmax(counts)\n","    dominant_color = codes[index_max]\n","\n","    return dominant_color\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description='Help to set arguments for Cloth Segmentation.')\n","    parser.add_argument('--image', type=str, help='Path to the input image')\n","    parser.add_argument('--cuda', action='store_true', help='Enable CUDA (default: False)')\n","    parser.add_argument('--checkpoint_path', type=str, default='model/cloth_segm.pth', help='Path to the checkpoint file')\n","    args = parser.parse_args()\n","\n","    main(args)\n","\n","    dominant_color = np.round(get_dominant_color(image_path))\n","    print(f\"The dominant color is RGB: {dominant_color}\")\n","\n","    rgb_value = dominant_color\n","    print(convert_rgb_to_name(np.round(rgb_value)))\n","\n"]}]}